{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCc3XZEyG3XV"
   },
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 2, Sprint 3, Module 4*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Model Interpretation 2\n",
    "\n",
    "You will use your portfolio project dataset for all assignments this sprint.\n",
    "\n",
    "## Assignment\n",
    "\n",
    "Complete these tasks for your project, and document your work.\n",
    "\n",
    "- [ ] Continue to iterate on your project: data cleaning, exploratory visualization, feature engineering, modeling.\n",
    "- [ ] Make a Shapley force plot to explain at least 1 individual prediction.\n",
    "- [ ] Share at least 1 visualization (of any type) on Slack.\n",
    "\n",
    "But, if you aren't ready to make a Shapley force plot with your own dataset today, that's okay. You can practice this objective with another dataset instead (any dataset you've worked with previously). An example solution will be provided for the Titanic dataset, which is in the data directory of this repository.\n",
    "\n",
    "**Multi-class classification** will result in multiple sets of Shapley Values (one for each class).\n",
    "\n",
    "\n",
    "## Stretch Goals\n",
    "- [ ] Make Shapley force plots to explain at least 4 individual predictions.\n",
    "    - If your project is Binary Classification, you can do a True Positive, True Negative, False Positive, False Negative.\n",
    "    - If your project is Regression, you can do a high prediction with low error, a low prediction with low error, a high prediction with high error, and a low prediction with high error.\n",
    "- [ ] Use Shapley values to display verbal explanations of individual predictions.\n",
    "- [ ] Use the SHAP library for other visualization types.\n",
    "\n",
    "The [SHAP repo](https://github.com/slundberg/shap) has examples for many visualization types, including:\n",
    "\n",
    "- Force Plot, individual predictions\n",
    "- Force Plot, multiple predictions\n",
    "- Dependence Plot\n",
    "- Summary Plot\n",
    "- Summary Plot, Bar\n",
    "- Interaction Values\n",
    "- Decision Plots\n",
    "\n",
    "We just did the first type during the lesson. The [Kaggle microcourse](https://www.kaggle.com/dansbecker/advanced-uses-of-shap-values) shows two more. Experiment and see what you can learn!\n",
    "\n",
    "\n",
    "## Links\n",
    "- [Kaggle / Dan Becker: Machine Learning Explainability — SHAP Values](https://www.kaggle.com/learn/machine-learning-explainability)\n",
    "- [Christoph Molnar: Interpretable Machine Learning — Shapley Values](https://christophm.github.io/interpretable-ml-book/shapley.html)\n",
    "- [SHAP repo](https://github.com/slundberg/shap) & [docs](https://shap.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# If you're on Colab:\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Applied-Modeling/master/data/'\n",
    "    !pip install category_encoders==2.*\n",
    "    !pip install eli5\n",
    "    !pip install pdpbox\n",
    "    !pip install shap\n",
    "\n",
    "# If you're working locally:\n",
    "else:\n",
    "    DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'category_encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b4ceab68e280>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimpute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
     ]
    }
   ],
   "source": [
    "# all imports needed for this sheet\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(DATA_PATH+'/Unit_2_project_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_reasons = ['Rental by client with RRH or equivalent subsidy', \n",
    "                'Rental by client, no ongoing housing subsidy', \n",
    "                'Staying or living with family, permanent tenure', \n",
    "                'Rental by client, other ongoing housing subsidy',\n",
    "                'Permanent housing (other than RRH) for formerly homeless persons', \n",
    "                'Staying or living with friends, permanent tenure', \n",
    "                'Owned by client, with ongoing housing subsidy', \n",
    "                'Rental by client, VASH housing Subsidy'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # create target column (multiple types of exits to perm)\n",
    "df['perm_leaver'] = df['3.12 Exit Destination'].isin(exit_reasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace spaces with underscore\n",
    "df.columns = df.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = df\n",
    "\n",
    "# Split train into train & test\n",
    "train, test = train_test_split(train, train_size=0.80, test_size=0.20, \n",
    "                              stratify=train['perm_leaver'], random_state=42)\n",
    "\n",
    "def wrangle(X):\n",
    "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
    "    \n",
    "    # Prevent SettingWithCopyWarning\n",
    "    X = X.copy()\n",
    "    \n",
    "    # drop any private information\n",
    "    X = X.drop(columns=['3.1_FirstName', '3.1_LastName', '3.2_SocSecNo', \n",
    "                      '3.3_Birthdate', 'V5_Prior_Address'])\n",
    "    \n",
    "    # drop unusable columns\n",
    "    X = X.drop(columns=['2.1_Organization_Name', '2.4_ProjectType',\n",
    "                        'WorkSource_Referral_Most_Recent', 'YAHP_Referral_Most_Recent',\n",
    "                        'SOAR_Enrollment_Determination_(Most_Recent)',\n",
    "                        'R7_General_Health_Status', 'R8_Dental_Health_Status',\n",
    "                        'R9_Mental_Health_Status', 'RRH_Date_Of_Move-In',\n",
    "                        'RRH_In_Permanent_Housing', 'R10_Pregnancy_Due_Date',\n",
    "                        'R10_Pregnancy_Status', 'R1_Referral_Source',\n",
    "                        'R2_Date_Status_Determined', 'R2_Enroll_Status',\n",
    "                        'R2_Reason_Why_No_Services_Funded', 'R2_Runaway_Youth',\n",
    "                        'R3_Sexual_Orientation', '2.5_Utilization_Tracking_Method_(Invalid)',\n",
    "                        '2.2_Project_Name', '2.6_Federal_Grant_Programs', '3.16_Client_Location',\n",
    "                        '3.917_Stayed_Less_Than_90_Days', \n",
    "                        '3.917b_Stayed_in_Streets,_ES_or_SH_Night_Before', \n",
    "                        '3.917b_Stayed_Less_Than_7_Nights', '4.24_In_School_(Retired_Data_Element)',\n",
    "                        'CaseChildren', 'ClientID', 'HEN-HP_Referral_Most_Recent',\n",
    "                        'HEN-RRH_Referral_Most_Recent', 'Emergency_Shelter_|_Most_Recent_Enrollment',\n",
    "                        'ProgramType', 'Days_Enrolled_Until_RRH_Date_of_Move-in',\n",
    "                        'CurrentDate', 'Current_Age', 'Count_of_Bed_Nights_-_Entire_Episode',\n",
    "                        'Bed_Nights_During_Report_Period'])\n",
    "        \n",
    "    # drop rows with no exit destination (current guests at time of report)\n",
    "    X = X.dropna(subset=['3.12_Exit_Destination'])\n",
    "    \n",
    "    # remove columns to avoid data leakage\n",
    "    X = X.drop(columns=['3.12_Exit_Destination', '5.9_Household_ID', '5.8_Personal_ID',\n",
    "                       '4.2_Income_Total_at_Exit', '4.3_Non-Cash_Benefit_Count_at_Exit'])\n",
    "    \n",
    "    # Drop needless feature\n",
    "    unusable_variance = ['Enrollment_Created_By', '4.24_Current_Status_(Retired_Data_Element)']\n",
    "    X = X.drop(columns=unusable_variance)\n",
    "\n",
    "    # Drop columns with timestamp\n",
    "    timestamp_columns = ['3.10_Enroll_Date', '3.11_Exit_Date', \n",
    "                         'Date_of_Last_ES_Stay_(Beta)', 'Date_of_First_ES_Stay_(Beta)', \n",
    "                         'Prevention_|_Most_Recent_Enrollment', 'PSH_|_Most_Recent_Enrollment', \n",
    "                         'Transitional_Housing_|_Most_Recent_Enrollment', 'Coordinated_Entry_|_Most_Recent_Enrollment', \n",
    "                         'Street_Outreach_|_Most_Recent_Enrollment', 'RRH_|_Most_Recent_Enrollment', \n",
    "                         'SOAR_Eligibility_Determination_(Most_Recent)', 'Date_of_First_Contact_(Beta)',\n",
    "                         'Date_of_Last_Contact_(Beta)', '4.13_Engagement_Date', '4.11_Domestic_Violence_-_When_it_Occurred',\n",
    "                         '3.917_Homeless_Start_Date']\n",
    "    X = X.drop(columns=timestamp_columns)\n",
    "    \n",
    "    # drop rows with no bednights\n",
    "    X = X.dropna(subset=['Days_Enrolled_in_Project', \n",
    "           'Count_of_Bed_Nights_(Housing_Check-ins)', \n",
    "          'Age_at_Enrollment', 'CaseMembers',\n",
    "          '4.12_Contact_Services'])\n",
    "    \n",
    "    # return the wrangled dataframe\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = wrangle(train)\n",
    "test = wrangle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign to X, y\n",
    "features = ['Days_Enrolled_in_Project', \n",
    "           'Age_at_Enrollment', 'CaseMembers'\n",
    "          ]\n",
    "target = 'perm_leaver'\n",
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "X_test = test[features]\n",
    "y_test = test[target]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = { \n",
    "    'n_estimators': randint(50, 500), \n",
    "    'max_depth': [5, 10, 15, 20, None], \n",
    "    'max_features': uniform(0, 1), \n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42), \n",
    "    param_distributions=param_distributions, \n",
    "    n_iter=5, \n",
    "    cv=2, \n",
    "    scoring='neg_mean_absolute_error', \n",
    "    verbose=10, \n",
    "    return_train_score=True, \n",
    "    n_jobs=-1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best hyperparameters', search.best_params_)\n",
    "print('Cross-validation MAE', -search.best_score_)\n",
    "model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an individual observation to explain.\n",
    "# For example, the 0th row from the test set.\n",
    "row = X_test.iloc[[0]]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did this guest exit to perm housing?\n",
    "y_test.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the model predict for this guest?\n",
    "model.predict(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why did the model predict this?\n",
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(row)\n",
    "\n",
    "shap.initjs()\n",
    "shap.force_plot(\n",
    "    base_value=explainer.expected_value,\n",
    "    shap_values=shap_values,\n",
    "    features=row\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Days_Enrolled_in_Project,  \n",
    "            Age_at_Enrollment, CaseMembers, model):\n",
    "\n",
    "  # Make df from inputs\n",
    "  df = pd.DataFrame(\n",
    "      data=[[Days_Enrolled_in_Project, \n",
    "            Age_at_Enrollment, CaseMembers,            \n",
    "             ]],\n",
    "      columns=['Days_Enrolled_in_Project', \n",
    "              'Age_at_Enrollment', 'CaseMembers']      \n",
    "  )\n",
    "\n",
    "  # Make a prediction\n",
    "  pred = model.predict(df)[0]\n",
    "  \n",
    "  # Calculate the shap values\n",
    "  explainer = shap.TreeExplainer(model)\n",
    "  shap_values = explainer.shap_values(df)\n",
    "\n",
    "  # Print some results\n",
    "  feature_names = df.columns\n",
    "  feature_values = df.values[0]\n",
    "  shaps = pd.Series(shap_values[0], zip(feature_names, feature_values))\n",
    "\n",
    "  result = f'${pred:,.0f} estimate exit destination for this guest. \\n\\n'\n",
    "  result += f'Starting from baseline of {explainer.expected_value:,.0f} \\n'\n",
    "  result += shaps.to_string()\n",
    "  print(result)\n",
    "\n",
    "  # Show the shapley force plot\n",
    "  shap.initjs()\n",
    "  return shap.force_plot(\n",
    "    base_value=explainer.expected_value,\n",
    "    shap_values=shap_values,\n",
    "    features=df\n",
    ")\n",
    "  \n",
    "predict(30, 20, 3, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(10, 10, 2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
